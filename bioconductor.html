<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Bioconductor Pipeline</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}

.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Ro Allen</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="blog.html">Blog</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Pipelines
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="bioconductor.html">Bioconductor</a>
    </li>
    <li>
      <a href="qiime2.html">QIIME2</a>
    </li>
  </ul>
</li>
<li>
  <a href="coda.html">CoDa</a>
</li>
<li>
  <a href="publications.html">Publications</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Bioconductor Pipeline</h1>

</div>


<p>The <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4955027.2/">bioconductor pipeline</a> is an amplicon sequencing pipeline which can be fully implemented in the R environment. The pipeline relies on the denoising alogrith <a href="https://www.nature.com/articles/nmeth.3869?WT.feed_name=subjects_statistical-methods">DADA2</a> to generate a table of specific amplicon sequence variants (ASVs) rather than clusered operational taxoonmic units (OTUs). This page provides a complete walkthrough of the pipeline, using illumina MiSeq data from the <em>GEOTRACES</em> ocean acidification experiment. This page is modified from a <a href="https://benjjneb.github.io/dada2/tutorial.html">tutorial produced by the pipeline authors</a>.</p>
<div id="install-dada2" class="section level3">
<h3>Install DADA2</h3>
<p>The first stage is to install DADA2 if it is not already available on the machine.</p>
<pre class="r"><code>source(&quot;https://bioconductor.org/biocLite.R&quot;)
biocLite(&quot;dada2&quot;)
biocLite(&quot;phyloseq&quot;)</code></pre>
<pre class="r"><code>library(dada2)
library(phyloseq)
library(ggplot2)</code></pre>
</div>
<div id="read-files" class="section level3">
<h3>Read files</h3>
<p>.fastq.gz files must be unzipped to .fastq files prior to processing in the dada2 pipeline. Unzipped files should be saved in a specific folder. Note that unzipping these files will use a substantial amount of memory. <em>Reading files will fail if they are not unzipped</em>.</p>
<p>Reading files into dada2 first requires an appropriate path to be set. This should lead to the folder in which the raw .fastq files for your project are saved. It is important that only .fastq files from your specific project are in this folder, as the script will read all files in this folder. We confirm that we have set the path to the correct location using the list.files command.</p>
<pre class="r"><code>path = &quot;~/geotraces-exp/Geotraces_exp_fastq&quot;
# check the file path is correct using list.files(path).

# we take this opportunity to extract the file names as a
# vector, which will be used later in the script
f.names = as.vector(list.files(path, pattern = &quot;_R1_001.fastq&quot;, 
    full.names = F))
r.names = as.vector(list.files(path, pattern = &quot;_R2_001.fastq&quot;, 
    full.names = F))

# pattern identifies all files in the path ending in
# &#39;_R1_001.fastq&#39;. This orders and separates forward and
# reverse reads.
fnFs = sort(list.files(path, pattern = &quot;_R1_001.fastq&quot;, full.names = TRUE))
fnRs = sort(list.files(path, pattern = &quot;_R2_001.fastq&quot;, full.names = TRUE))</code></pre>
</div>
<div id="plot-quality-scores-from-.fastq-files" class="section level3">
<h3>Plot quality scores from .fastq files</h3>
<p>Plot the Q scores associated with each .fastq file. Look for drop offs in sequence quality, whilst considering that a QS of 30 is equivalent to a 99.9% probability of accurate base calling and a QS of 20 is equivalent to a 99% probaility of accurate base calling. The number of plots is 36 in this case, as there are 36 samples; this should be adjusted to the number of samples in your investigation.</p>
<pre class="r"><code>qp.f = plotQualityProfile(fnFs[1:36])
qp.r = plotQualityProfile(fnRs[1:36])

# illustration of plot outputs (first 2 only)
plotQualityProfile(fnFs[1:2])</code></pre>
<p>The gray block in the background of the plots represent the density of sequences in that sample which have a quality score at that position. This should result in a dark grey band &gt;30 QS, and a much more pale distribution of blocks below. The solid green line represents the mean QS, the solid orange line represents the median QS, and the dashed orange lines represent the 25th and 75th quantiles.</p>
</div>
<div id="filter-and-trim-quality-control" class="section level3">
<h3>Filter and trim (quality control)</h3>
<p>The first step in the filtering and trimming script is to setup a new directory for the filtered and trimed .fastq files to be saved in. This is achieved using the <em>file.path()</em> function to create a subdirectory in the <em>path</em> location. We then create file paths for filtered forward and reverse reads respectively, with a predetermined suffix which allows us to differentiate them (“_F_filt.fastq.gz&quot; / “_R_filt.fastq.gz“)</p>
<p>After inspecting the quality plots, we can determine the trim and truncate parameters for .fastq files.</p>
<p><em>Check raw .fastq files for primer sequences, and remove using the trim function if necessary</em></p>
<p>In this instance, forward and reverse primers are still present. The forward primer is 19 bp, whilst the reverse primer is 20 bp, these will be removed by setting the trim parameters to c(19,20). The truncation length should be set to cut off where any signfiicant drop off in quality occurs, but it is advisable to remove approximately the last 10 bp anyway as these reads can be unreliable.</p>
<pre class="r"><code>filt_path = file.path(path, &quot;filtered&quot;)  #place filtered files in &#39;filtered&#39; subdirectory
filtFs = file.path(filt_path, paste0(f.names, &quot;_F_filt.fastq.gz&quot;))
filtRs = file.path(filt_path, paste0(r.names, &quot;_R_filt.fastq.gz&quot;))

out &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen = c(240, 
    181), trimLeft = c(19, 20), maxN = 0, maxEE = c(2, 2), truncQ = 2, 
    rm.phix = TRUE, compress = TRUE, multithread = TRUE)

head(out)  #check the results</code></pre>
</div>
<div id="learning-error-rates" class="section level3">
<h3>Learning error rates</h3>
<p>The dada2 algorithm relies on a error model calculated from each amplicon dataset. This model is then used downstream to distinguish true sequence variants from sequence variants which have arisen from sequencing errors (using a probability approach). From the pipeline authors: <em>“The learnErrors method learns the error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many optimization problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).”</em></p>
<pre class="r"><code># calculating the error model
errF = learnErrors(filtFs, multithread = TRUE)
errR = learnErrors(filtRs, multithread = TRUE)

# plotting errors
plot.errF = plotErrors(errF, nominalQ = TRUE) + theme_bw() + 
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
plot.errR = plotErrors(errR, nominalQ = TRUE) + theme_bw() + 
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())</code></pre>
<p>The error rates for each possible between-base call are shown. The red line represents the expected error according to the Q value (e.g. 99.9% accuracy for QS 30). The black line represents the estimated error rate, fitted to the observed error rates (points). Estimated error rates should decrease with increasing quality scores. Note that this algorithm calculates errors based on the first 1M reads, to reduce computational demand. If your error plots look unreasonable (e.g. a trend of increasing error with increasing quality score) you can manually increase the number of reads to consider (increasing computational demand, and therefore runtime) using the <em>nreads</em> variable in the <em>learnErrors</em> function.</p>
</div>
<div id="dereplicate-sequences" class="section level3">
<h3>Dereplicate sequences</h3>
<p>Dereplication merges all identical sequence reads into a new file containing only unique sequences, with a corresponding abundance metadata. This is done to reduce the computational demand of calculations downstream by removing redundant comparisons. The pipeline then runs signficantly faster.</p>
<p>Sequence quality scores are retained during this process, as a consensus quality profile is generate from all identical sequences being dereplicated to a single unique sequence. The quality profiles are then used to inform the error model of the denoising step, which increases dada2’s accuracy.</p>
<pre class="r"><code>derepFs = derepFastq(filtFs, verbose = TRUE)
derepRs = derepFastq(filtRs, verbose = TRUE)
names(derepFs) = f.names
names(derepRs) = r.names</code></pre>
</div>
<div id="dada2-core-sequence-variant-inference-algorithm" class="section level3">
<h3>DADA2 core sequence variant inference algorithm</h3>
<p>At this stage the DADA2 takes all the information provided in the dereplicated sequences and the error models to determine which sequences represent true amplicon sequence variants, and which sequences have arisen from errors. Thus we end up with a list of true amplicon sequence variants, avoiding the archaic practice of clustering.</p>
<pre class="r"><code>dadaFs = dada(derepFs, err = errF, multithread = TRUE)
dadaRs = dada(derepRs, err = errR, multithread = TRUE)
dadaFs[[1]]
dadaRs[[1]]</code></pre>
<p>By calling <em>dadaFs[[1]]</em> the number of real sequences (named sample sequences) infered from the number of unique input sequences are displayed for sample number 1. There is much more information available which can be accessed through <em>help(“dada-class”)</em>.</p>
</div>
<div id="merge-paired-reads" class="section level3">
<h3>Merge paired reads</h3>
<p>The forward and reverse reads are then merged using the <em>mergePairs</em> function. Note that the forward and reverse reads must be in matching order at this stage.</p>
<pre class="r"><code>mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE)

# Check merger success by sample ([[1]] refers to sample 1)
head(mergers[[1]])</code></pre>
</div>
<div id="generate-sequence-table" class="section level3">
<h3>Generate sequence table</h3>
<p>A sequence table can be constructed from the merged sequence file. Viewing this table will reveal a number of amplicon lengths which are outside the expected range of your region, these are likely to be erroneous.</p>
<pre class="r"><code>seqtab = makeSequenceTable(mergers)
dim(seqtab)
table(nchar(getSequences(seqtab)))</code></pre>
</div>
<div id="plot-amplicon-length-distribution" class="section level3">
<h3>Plot amplicon length distribution</h3>
<p>Plotting amplicon length frequency allows us to visualise the distribution of amplicon sizes, informing the amplicon length to conserve versus lengths to discard.</p>
<pre class="r"><code>ex = as.data.frame(table(nchar(getSequences(seqtab))))
ex.plot = ggplot(ex, aes(Var1, Freq)) + geom_bar(stat = &quot;identity&quot;, 
    aes(fill = Var1)) + ylab(&quot;Frequency&quot;) + xlab(&quot;Merged Sequence Length&quot;) + 
    scale_x_discrete(breaks = c(220, 240, 250, 260, 270, 300, 
        330, 360)) + theme_bw() + theme(panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank()) + theme(legend.position = &quot;none&quot;)
ex.plot</code></pre>
</div>
<div id="excise-amplicons-of-desired-length" class="section level3">
<h3>Excise amplicons of desired length</h3>
<p>Here we select only amplicons between 250 and 256bp in length to carry forward.</p>
<pre class="r"><code>seqtab.ex = seqtab[, nchar(colnames(seqtab)) %in% seq(250, 256)]
table(nchar(getSequences(seqtab.ex)))</code></pre>
</div>
<div id="remove-chimeras" class="section level3">
<h3>Remove chimeras</h3>
<p>Note that if you lose a large number of sequence reads at this stage, it is likely that primer sequences still remain in your reads. These should have been removed manually at the trim and truncate stage.</p>
<pre class="r"><code>seqtab.ex.chi = removeBimeraDenovo(seqtab.ex, method = &quot;consensus&quot;, 
    multithread = T, verbose = T)</code></pre>
</div>
<div id="track-sequence-loss" class="section level3">
<h3>Track sequence loss</h3>
<p>Here we track the number of sequence reads per sample remaining after each stage of processing. This provides an opportunity to identify points in the pipeline where large proportions of sequences were lost, which may be worth revisiting and tweaking. Commonly, 15-20% of reads are lost over the course of the pipeline.</p>
<pre class="r"><code>getN = function(x) sum(getUniques(x))
track = cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), 
    rowSums(seqtab.ex), rowSums(seqtab.ex.chi))
colnames(track) = c(&quot;input&quot;, &quot;filtered&quot;, &quot;denoised&quot;, &quot;merged&quot;, 
    &quot;tabled&quot;, &quot;no chim&quot;)
rownames(track) = f.names
head(track)</code></pre>
</div>
<div id="assign-taxonomy" class="section level3">
<h3>Assign taxonomy</h3>
<p>It is important to consider which taxonomic database to use for this step. The two most common options are SILVA and Greengenes. Typically, Greengenes offers more reliable taxonomic classifications, but has less coverage, and is not updated regularly (last update ~2014). SILVA is a curate database which is updated frequently, but can be less reliable than Greengenes in certain cases. Some studies use integrated approaches where taxonomies are assigned from numerous databases, and checked for agreement. The bioconductor pipeline team maintain taxonomic classfiers from the SILVA, Greengenes, and RDP databases for the Earth Microbiome Project (EMP) primers 515-F (or Y for marine samples) &amp; 806R. In this case, these classifiers can be <a href="https://benjjneb.github.io/dada2/training.html">downloaded directly</a>.</p>
<p>Once you have downloaded your desired classifier, place it in a folder, and call the specific file path in the <em>assignTaxonomy</em> function. This step may take a significant amount of time, depending of computing power, sample number, and sequencing depth.</p>
<pre class="r"><code>gg.taxa = assignTaxonomy(seqtab.ex.chi, &quot;~/geotraces-exp/Geotraces_exp_unzipped/gg_13_8_train_set_97.fa&quot;, 
    multithread = T)</code></pre>
</div>
<div id="upload-metadata" class="section level3">
<h3>Upload metadata</h3>
<p>The metadata file associated with your sequences should be uploaded with samples as rows and variables as columns. Various file formats can be supported with the ultimate goal of creating a metadata dataframe in R which can then be incorporated into a phyloseq object. It is critical that the sample order in your metadata table matches the sample order in your sequence table (in this case <em>seqtab.ex.chi</em>).</p>
<pre class="r"><code>sd.geo = read.csv(&quot;~/geotraces-exp/geo-mapping.csv&quot;)
head(sd.geo)
# if your variable names (column names) are unclear, rename
# them at this stage.</code></pre>
</div>
<div id="rename-samples-and-asvs" class="section level3">
<h3>Rename samples and ASVs</h3>
<p>Before we construct the phyloseq object, it is important to rename all the samples (which will currently named by their file name) and ASVs (which will currently be named by their full sequence). This will make handling the phyloseq object easier downstream.</p>
<pre class="r"><code># create a vector for sample names
s.vec = as.vector(1:36)  #number should reflect your total number of samples
s.nam = cbind(&quot;sample_&quot;, s.vec)
s.nam = as.data.frame(s.nam)
s.names = paste0(s.nam$V1, s.nam$s.vec)
s.names = as.data.frame(s.names)

# apply sample names to metadata
row.names(sd.geo) = s.names$s.names
sd.geo = as.data.frame(sd.geo)
head(sd.geo)

# apply sample names to sequence table
row.names(seqtab.ex.chi) = s.names$s.names

# create vector for ASV names
a.vec = as.vector(1:1937)  #number should reflect your total ASVs
a.nam = cbind(&quot;asv_&quot;, a.vec)
a.nam = as.data.frame(a.nam)
asv.names = paste0(a.nam$V1, a.nam$a.vec)
asv.names = as.data.frame(asv.names)

# apply ASV names to sequence table
row.names(seqtab.ex.chi) = asv.names$asv.names
head(seqtab.ex.chi)</code></pre>
</div>
<div id="construct-a-phyloseq-object" class="section level3">
<h3>Construct a phyloseq object</h3>
<p>Finally, a phyloseq object is created which can be handled easily for statistical analyses. The phyloseq object from this tutorial is notably missing a phylogenetic tree. This can easily be constructed and will be added to this tutorial at a later date.</p>
<pre class="r"><code>geo.ex = phyloseq(tax_table(gg.taxa), otu_table(seqtab.ex.chi, 
    taxa_are_rows = FALSE), sample_data(sd.geo))
geo.ex</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
