---
title: 'Bioconductor Pipeline'
---

The [bioconductor pipeline](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4955027.2/) is an amplicon sequencing pipeline which can be fully implemented in the R environment. The pipeline relies on the denoising alogrith [DADA2](https://www.nature.com/articles/nmeth.3869?WT.feed_name=subjects_statistical-methods) to generate a table of specific amplicon sequence variants (ASVs) rather than clusered operational taxoonmic units (OTUs). This page provides a complete walkthrough of the pipeline, using illumina MiSeq data from the *GEOTRACES* ocean acidification experiment. This page is modified from a [tutorial produced by the pipeline authors](https://benjjneb.github.io/dada2/tutorial.html).

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy=TRUE)
```


##Install dada2
The first stage is to install dada2 if it is not already available on the machine. 
```{r, eval=FALSE}
source("https://bioconductor.org/biocLite.R")
biocLite("dada2")
biocLite("phyloseq")
```

```{r, eval=FALSE}
library(dada2)
library(phyloseq)
library(ggplot2)
```

##Unzip .fastq.gz files
.fastq.gz files must be unzipped to .fastq files prior to processing in the dada2 pipeline. Unzipped files should be saved in a specific folder. Note that unzipping these files will use a substantial amount of memory. 

##Read files
Reading files into dada2 first requires an appropriate path to be set. This should lead to the folder in which the raw .fastq files for your project are saved. It is important that only .fastq files from your specific project are in this folder, as the script will read all files in this folder. We confirm that we have set the path to the correct location using the list.files command. 

```{r, eval=FALSE}
path = "~/geotraces-exp/Geotraces_exp_fastq"
#check the file path is correct using list.files(path).

#we take this opportunity to extract the file names as a vector, which will be used later in the script
f.names = as.vector(list.files(path, pattern = "_R1_001.fastq", full.names = F))
r.names = as.vector(list.files(path, pattern = "_R2_001.fastq", full.names = F))

#pattern identifies all files in the path ending in "_R1_001.fastq". This orders and separates forward and reverse reads. 
fnFs = sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs = sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
```

##Plot quality scores from .fastq files
Plot the Q scores associated with each .fastq file. Look for drop offs in sequence quality, whilst considering that a QS of 30 is equivalent to a 99.9% probability of accurate base calling and a QS of 20 is equivalent to a 99% probaility of accurate base calling. The number of plots is 36 in this case, as there are 36 samples; this should be adjusted to the number of samples in your investigation.
```{r, eval=FALSE}
qp.f = plotQualityProfile(fnFs[1:36])
qp.r = plotQualityProfile(fnRs[1:36])

#illustration of plot outputs (first 2 only)
plotQualityProfile(fnFs[1:2])
```

###Interpreting quality profile plots
The gray block in the background of the plots represent the density of sequences in that sample which have a quality score at that position. This should result in a dark grey band >30 QS, and a much more pale distribution of blocks below. The solid green line represents the mean QS, the solid orange line represents the median QS, and the dashed orange lines represent the 25th and 75th quantiles.

##Filtering and trimming (quality control)
The first step in the filtering and trimming script is to setup a new directory for the filtered and trimed .fastq files to be saved in. This is achieved using the *file.path()* function to create a subdirectory in the *path* location. We then create file paths for filtered forward and reverse reads respectively, with a predetermined suffix which allows us to differentiate them ("_F_filt.fastq.gz" / "_R_filt.fastq.gz")

After inspecting the quality plots, we can determine the trim and truncate parameters for .fastq files. 

  *Check raw .fastq files for primer sequences, and remove using the trim function if necessary*

In this instance, forward and reverse primers are still present. The forward primer is 19 bp, whilst the reverse primer is 20 bp, these will be removed by setting the trim parameters to c(19,20). The truncation length should be set to cut off where any signfiicant drop off in quality occurs, but it is advisable to remove approximately the last 10 bp anyway as these reads can be unreliable. 
```{r, eval=FALSE}
filt_path = file.path(path, "filtered") #place filtered files in "filtered" subdirectory
filtFs = file.path(filt_path, paste0(f.names, "_F_filt.fastq.gz"))
filtRs = file.path(filt_path, paste0(r.names, "_R_filt.fastq.gz"))

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,181), trimLeft=c(19,20),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)

head(out) #check the results
```

##Learning error rates
The dada2 algorithm relies on a error model calculated from each amplicon dataset. This model is then used downstream to distinguish true sequence variants from sequence variants which have arisen from sequencing errors (using a probability approach). From the pipeline authors: *"The learnErrors method learns the error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many optimization problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors)."*

```{r, eval=FALSE}
#calculating the error model
errF = learnErrors(filtFs, multithread=TRUE)
errR = learnErrors(filtRs, multithread=TRUE)

#plotting errors
plot.errF = plotErrors(errF, nominalQ=TRUE) + theme_bw() + theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())
plot.errR = plotErrors(errR, nominalQ=TRUE) + theme_bw() + theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())
```

###Interpreting error plots
The error rates for each possible between-base call are shown. The red line represents the expected error according to the Q value (e.g. 99.9% accuracy for QS 30). The black line represents the estimated error rate, fitted to the observed error rates (points). Estimated error rates should decrease with increasing quality scores. Note that this algorithm calculates errors based on the first 1M reads, to reduce computational demand. If your error plots look unreasonable (e.g. a trend of increasing error with increasing quality score) you can manually increase the number of reads to consider (increasing computational demand, and therefore runtime) using the *nreads* variable in the *learnErrors* function.

##Dereplicating sequences
Dereplication merges all identical sequence reads into a new file containing only unique sequences, with a corresponding abundance metadata. This is done to reduce the computational demand of calculations downstream by removing redundant comparisons. The pipeline then runs signficantly faster. 

Sequence quality scores are retained during this process, as a consensus quality profile is generate from all identical sequences being dereplicated to a single unique sequence. The quality profiles are then used to inform the error model of the denoising step, which increases dada2's accuracy.

```{r, eval=FALSE}
derepFs = derepFastq(filtFs, verbose=TRUE)
derepRs = derepFastq(filtRs, verbose=TRUE)
names(derepFs) = f.names
names(derepRs) = r.names
```

##Dada2 core sequence variant inference algorithm
At this stage the dada2 takes all the information provided in the dereplicated sequences and the error models to determine which sequences represent true amplicon sequence variants, and which sequences have arisen from errors. Thus we end up with a list of true amplicon sequence variants, avoiding the archaic practice of clustering. 

```{r, eval=FALSE}
dadaFs = dada(derepFs, err=errF, multithread=TRUE)
dadaRs = dada(derepRs, err=errR, multithread=TRUE)
dadaFs[[1]]
dadaRs[[1]]
```

###Interpreting the dada2 output
By calling *dadaFs[[1]]* the number of real sequences (named sample sequences) infered from the number of unique input sequences are displayed for sample number 1. There is much more information available which can be accessed through *help("dada-class")*.

##Merge paired reads
The forward and reverse reads are then merged using the *mergePairs* function. Note that the forward and reverse reads must be in matching order at this stage. 

```{r, eval=FALSE}
mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE)

#Check merger success by sample ([[1]] refers to sample 1)
head(mergers[[1]]) 
```

##Create sequence table
A sequence table can be constructed from the merged sequence file. Viewing this table will reveal a number of amplicon lengths which are outside the expected range of your region, these are likely to be erroneous.
```{r, eval=FALSE}
seqtab = makeSequenceTable(mergers)
dim(seqtab)
table(nchar(getSequences(seqtab))) 
```

##Plot amplicon length frequency
Plotting amplicon length frequency allows us to visualise the distribution of amplicon sizes, informing the amplicon length to conserve versus lengths to discard. 
```{r, eval=FALSE}
ex = as.data.frame(table(nchar(getSequences(seqtab))))
ex.plot = ggplot(ex, aes(Var1, Freq)) + geom_bar(stat = "identity", aes(fill = Var1)) + ylab("Frequency") + xlab("Merged Sequence Length") + scale_x_discrete(breaks = c(220,240,250,260,270,300,330,360)) + theme_bw() + theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank()) + theme(legend.position="none")
ex.plot
```

##Excise amplicons of desired length
Here we select only amplicons between 250 and 256bp in length to carry forward.
```{r, eval=FALSE}
seqtab.ex = seqtab[,nchar(colnames(seqtab)) %in% seq(250,256)]
table(nchar(getSequences(seqtab.ex)))
```

##Remove chimeras
Note that if you lose a large number of sequence reads at this stage, it is likely that primer sequences still remain in your reads. These should have been removed manually at the trim and truncate stage. 
```{r, eval=FALSE}
seqtab.ex.chi = removeBimeraDenovo(seqtab.ex, method = 'consensus', multithread = T, verbose = T)
```

##Track sequence loss throughout pipeline
Here we track the number of sequence reads per sample remaining after each stage of processing. This provides an opportunity to identify points in the pipeline where large proportions of sequences were lost, which may be worth revisiting and tweaking. Commonly, 15-20% of reads are lost over the course of the pipeline.
```{r, eval=FALSE}
getN = function(x) sum(getUniques(x))
track = cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab.ex), rowSums(seqtab.ex.chi))
colnames(track) = c("input", "filtered", "denoised", "merged", "tabled", "no chim")
rownames(track) = f.names
head(track)
```

##Taxonomic assignment
It is important to consider which taxonomic database to use for this step. The two most common options are SILVA and Greengenes. Typically, Greengenes offers more reliable taxonomic classifications, but has less coverage, and is not updated regularly (last update ~2014). SILVA is a curate database which is updated frequently, but can be less reliable than Greengenes in certain cases. Some studies use integrated approaches where taxonomies are assigned from numerous databases, and checked for agreement. The bioconductor pipeline team maintain taxonomic classfiers from the SILVA, Greengenes, and RDP databases for the Earth Microbiome Project (EMP) primers 515-F (or Y for marine samples) & 806R. In this case, these classifiers can be downloaded directly from https://benjjneb.github.io/dada2/training.html.

Once you have downloaded your desired classifier, place it in a folder, and call the specific file path in the *assignTaxonomy* function. This step may take a significant amount of time, depending of computing power, sample number, and sequencing depth. 
```{r, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
gg.taxa = assignTaxonomy(seqtab.ex.chi, "~/geotraces-exp/Geotraces_exp_unzipped/gg_13_8_train_set_97.fa", multithread = T)
```

##Upload metadata
The metadata file associated with your sequences should be uploaded with samples as rows and variables as columns. Various file formats can be supported with the ultimate goal of creating a metadata dataframe in R which can then be incorporated into a phyloseq object. It is critical that the sample order in your metadata table matches the sample order in your sequence table (in this case *seqtab.ex.chi*).
```{r, eval=FALSE}
sd.geo = read.csv("~/geotraces-exp/geo-mapping.csv")
head(sd.geo)
#if your variable names (column names) are unclear, rename them at this stage.
```

##Rename samples and ASVs
Before we construct the phyloseq object, it is important to rename all the samples (which will currently named by their file name) and ASVs (which will currently be named by their full sequence). This will make handling the phyloseq object easier downstream. 
```{r, eval=FALSE}
#create a vector for sample names
s.vec = as.vector(1:36) #number should reflect your total number of samples
s.nam = cbind("sample_", s.vec)
s.nam = as.data.frame(s.nam)
s.names = paste0(s.nam$V1, s.nam$s.vec)
s.names = as.data.frame(s.names)

#apply sample names to metadata
row.names(sd.geo) = s.names$s.names 
sd.geo = as.data.frame(sd.geo)
head(sd.geo)

#apply sample names to sequence table
row.names(seqtab.ex.chi) = s.names$s.names

#create vector for ASV names
a.vec = as.vector(1:1937) #number should reflect your total ASVs
a.nam = cbind("asv_", a.vec)
a.nam = as.data.frame(a.nam)
asv.names = paste0(a.nam$V1, a.nam$a.vec)
asv.names = as.data.frame(asv.names)

#apply ASV names to sequence table
row.names(seqtab.ex.chi) = asv.names$asv.names
head(seqtab.ex.chi)
```

##Construct a phyloseq object
Finally, a phyloseq object is created which can be handled easily for statistical analyses. The phyloseq object from this tutorial is notably missing a phylogenetic tree. This can easily be constructed and will be added to this tutorial at a later date. 
```{r, eval=FALSE}
geo.ex = phyloseq(tax_table(gg.taxa), otu_table(seqtab.ex.chi, taxa_are_rows = FALSE), sample_data(sd.geo))
geo.ex
```

